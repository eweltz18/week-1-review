{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data\n",
    "\n",
    "First, we need to get the raw data. The 20 Newsgroups dataset is so popular that Scikit-Learn comes with some tools for getting the data. No scraping the web or unzipping data folders. We'll [preprocess the data](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html#filtering-text-for-more-realistic-training) by removing headers, footers, and quotes (which Scikit-Learn also handles for us). This gives us just the raw post text. For speed, we'll also take a random sample of 500 posts instead of all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "CATEGORIES = ('alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space')\n",
    "RM = ('headers', 'footers', 'quotes')\n",
    "\n",
    "def get_newsgroups(subset, remove=RM, categories=CATEGORIES, n=None):\n",
    "    raw = fetch_20newsgroups(subset=subset, remove=remove, categories=categories, shuffle=True)\n",
    "    n = n or len(raw.data)\n",
    "    return raw.data[:n], raw.target[:n]\n",
    "\n",
    "train_raw, train_y = get_newsgroups('train', n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw posts: 500\n",
      "Shape of train_y: (500,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of raw posts: {0}\".format(len(train_raw)))\n",
    "print(\"Shape of train_y: {0}\".format(train_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to define all of our operations on the training data as functions, since we need to apply the exact same sequence to process the test data. Our function returns a list of 500 strings (the raw post text) and a NumPy vector with 500 entries. `train_y[i]` is the category of `train_raw[i]`. So if `train_y[i]` is `0`, that means the true category of `train_raw[i]` is `alt.atheism`. It will take a minute or two to run the first time, since it needs to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "TRUE LABEL: 1\n"
     ]
    }
   ],
   "source": [
    "print(type(train_raw[0]))\n",
    "print(\"\\n{0}\\n\".format('-'*40))\n",
    "print(train_raw[0])\n",
    "print(\"\\n{0}\\n\".format('-'*40))\n",
    "print(\"TRUE LABEL: {0}\".format(train_y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured data\n",
    "\n",
    "Now it's time for spaCy. We need to parse the raw text (most importantly, tokenize it) so that we have a structured format we can create features with. We'll always want to do this, so let's just wrap `get_newsgroups(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def get_newsgroups_parsed(subset, nlp=nlp, remove=('headers', 'footers', 'quotes'), categories=CATEGORIES, n=None):\n",
    "    raw, y = get_newsgroups(subset, remove, categories, n)\n",
    "    return [nlp(text) for text in raw], y\n",
    "\n",
    "train_parsed, train_y = get_newsgroups_parsed('train', n=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "TRUE LABEL: 1\n"
     ]
    }
   ],
   "source": [
    "print(type(train_parsed[0]))\n",
    "print(\"\\n{0}\\n\".format('-'*40))\n",
    "print(train_parsed[0])\n",
    "print(\"\\n{0}\\n\".format('-'*40))\n",
    "print(\"TRUE LABEL: {0}\".format(train_y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "Our features will be bags-of-bigrams. That means we need to get the list of unigrams and bigrams that appear in each post. But first, we need to do some engineering. The following function will take a post and perform the following operations:\n",
    "\n",
    "* Filter out punctuation, numbers, etc. from unigrams and bigrams\n",
    "* Filter out stopwords (words with no semantic information like \"the\" or \"and\") from unigrams\n",
    "* Lowercase all words\n",
    "* Lemmatize all verbs (i.e. take the root)\n",
    "* Use the named entity tags from spaCy to replace all person names with a special word (`~~PERSON~~`) in bigrams\n",
    "\n",
    "This lets us generalize the information in the posts, so that our model won't overfit to the specific language used in the training set. The function isn't very general or efficient, but it illustrates the behavior we want pretty clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSON = '~~PERSON~~'\n",
    "\n",
    "# Check if is person, returns PERSON if wanted\n",
    "# Runs is_alpha filter on token, returns None if failed\n",
    "# Otherwise, lemmatizes if a verb and returns lowercased string\n",
    "def process_token(token, swap_person=False):\n",
    "    # Replace with person tag\n",
    "    if swap_person and token.ent_type_ == 'PERSON': return '~~PERSON~~'\n",
    "    # Run filter\n",
    "    if not token.is_alpha: return None\n",
    "    # Get lemma if verb\n",
    "    s = token.lemma_ if token.pos_.startswith('V') else token.string\n",
    "    # Lowercase\n",
    "    return s.strip().lower()\n",
    "\n",
    "# Get the list of unigrams and bigrams for each post\n",
    "def featurize_post(post):\n",
    "    unigrams, bigrams = [], []\n",
    "    for sentence in post.sents:\n",
    "        last_word, last_token = None, None\n",
    "        for token in sentence:\n",
    "            # First add unigram if not a stopword\n",
    "            word = process_token(token, swap_person=False)\n",
    "            if word is not None and not token.is_stop:\n",
    "                unigrams.append(word)\n",
    "            # Then add preceeding bigram if both aren't stopwords\n",
    "            if (word is not None) and (last_word is not None):\n",
    "                if not token.is_stop or not last_token.is_stop:\n",
    "                    bigrams.append((last_word, word))\n",
    "            last_word, last_token = word, token\n",
    "    return unigrams, bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's featurize all of the posts and collect all of the unigrams and bigrams in the training set to create a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unigrams, train_bigrams = zip(*[featurize_post(post) for post in train_parsed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "all_unigrams = list(set(gram for post in train_unigrams for gram in post))\n",
    "all_bigrams = list(set(gram for post in train_bigrams for gram in post))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram dictionary size (i.e. total number of unigram features): 8950\n",
      "\n",
      "First 15 entries:\n",
      "0   evangelism\n",
      "1   widely\n",
      "2   stratigraphic\n",
      "3   philosophical\n",
      "4   cat\n",
      "5   studies\n",
      "6   objectives\n",
      "7   formulas\n",
      "8   god\n",
      "9   envorinment\n",
      "10  notables\n",
      "11  inschrijv\n",
      "12  print\n",
      "13  sides\n",
      "14  deeper\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram dictionary size (i.e. total number of unigram features): {0}\".format(len(all_unigrams)))\n",
    "print(\"\\nFirst 15 entries:\")\n",
    "print(\"\\n\".join(\"{0:<4}{1}\".format(str(k), v) for k, v in enumerate(all_unigrams[:15])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram dictionary size (i.e. total number of bigram features): 30535\n",
      "\n",
      "First 15 entries:\n",
      "0   ('various', 'specs')\n",
      "1   ('the', 'grace')\n",
      "2   ('the', 'stability')\n",
      "3   ('of', 'christianity')\n",
      "4   ('interest', 'on')\n",
      "5   ('giant', 'fly')\n",
      "6   ('resultant', 'bitmap')\n",
      "7   ('printer', 'model')\n",
      "8   ('i', 'point')\n",
      "9   ('natives', 'be')\n",
      "10  ('still', 'hard')\n",
      "11  ('deltav', 'require')\n",
      "12  ('and', 'spring')\n",
      "13  ('the', 'precept')\n",
      "14  ('management', 'schemes')\n"
     ]
    }
   ],
   "source": [
    "print(\"Bigram dictionary size (i.e. total number of bigram features): {0}\".format(len(all_bigrams)))\n",
    "print(\"\\nFirst 15 entries:\")\n",
    "print(\"\\n\".join(\"{0:<4}{1}\".format(str(k), v) for k, v in enumerate(all_bigrams[:15])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-ngrams\n",
    "\n",
    "Let's start with a simple representation: a bag-of-ngrams. We'll create a unigram matrix $X_{\\text{unigram}}$ (dimensions $500 \\times 8950$) and a bigram matrix $X_{\\text{bigram}}$ (dimensions $500 \\times 30535$), then just stack them to create our final $X$ (dimensions $500 \\times 39485$). We'll write a couple functions to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Construct X_unigram or X_bigram\n",
    "def posts_to_matrix(posts, vocab):\n",
    "    # Initialize matrix\n",
    "    X = np.zeros((len(posts), len(vocab)))\n",
    "    for i, post in enumerate(posts):\n",
    "        # Get counts of ngrams in post that are also in vocab\n",
    "        post_filtered = [p for p in Counter(post).items() if p[0] in vocab]\n",
    "        if len(post_filtered) > 0:\n",
    "            keys, cts = zip(*post_filtered)\n",
    "            # Insert counts into row using vocabulary index\n",
    "            X[i, np.ravel([vocab[k] for k in keys])] = np.ravel(cts)\n",
    "    return X\n",
    "\n",
    "\n",
    "# Constucts X_unigram and X_bigram, then merges\n",
    "def construct_feature_matrix(unigrams, bigrams, unigram_vocab, bigram_vocab):\n",
    "    X_unigram = posts_to_matrix(unigrams, unigram_vocab)\n",
    "    X_bigram = posts_to_matrix(bigrams, bigram_vocab)\n",
    "    return np.hstack((X_unigram, X_bigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just construct the vocabularies as dictionaries mapping the word to their fixed index, then build the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_vocab = {unigram: i for i, unigram in enumerate(all_unigrams)}\n",
    "bigram_vocab = {bigram: i for i, bigram in enumerate(all_bigrams)}\n",
    "\n",
    "train_X = construct_feature_matrix(train_unigrams, train_bigrams, unigram_vocab, bigram_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:                     (500, 39485)\n",
      "# of non-zero entries in X:  66482\n",
      "% of non-zero entries in X:  0.34%\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:                     {0}\".format(train_X.shape))\n",
    "print(\"# of non-zero entries in X:  {0}\".format(np.sum(train_X != 0)))\n",
    "print(\"% of non-zero entries in X:  {0:.2f}%\".format(100 * np.mean(train_X != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix is very large, but quite sparse (almost all of the entries are zero). Our representation is very inefficient, but it's fine for our purposes. If we're training a simple softmax regression model, that means we have to train one parameter per ngram per category:\n",
    "\n",
    "$\\theta_{1,1}, \\theta_{1,2}, ...\\theta_{4, 39485}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\theta_{\\text{Atheism},\\text{ (able, )}}, \\theta_{\\text{Atheism},\\text{ (alert, )}}, ...\\theta_{\\text{Space}, \\text{ (zebra, run)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors\n",
    "\n",
    "Finally, let's create a different matrix format using word vectors: the 300 dimensional vectors we use to represent words. We'll just use unigrams for this one. We'll first construct the embedding table: the list of all the embeddings we care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_embedding_table(vocab, nlp):\n",
    "    return np.vstack([nlp(word).vector for word in vocab])\n",
    "\n",
    "U = build_embedding_table(all_unigrams, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll write a function to collect all of the embeddings in a post. The X we return will be a list, but we can think of it like a matrix with shape [Number of posts, Length of post, Embedding dimension]. For our purposes, this is [500, *Varies*, 300] since spaCy defaults to the 300-dimensional GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def posts_to_embedding_matrix(posts, vocab, embedding_table):\n",
    "    X = []\n",
    "    for post in posts:\n",
    "        post_filtered = np.ravel([vocab[word] for word in post if word in vocab])\n",
    "        if len(post_filtered) == 0:\n",
    "            X.append(np.zeros((1, embedding_table.shape[1])))\n",
    "        else:\n",
    "            X.append(embedding_table[post_filtered, :])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want one vector for each post, so we can just take the means down the columns of the matrices stored in `train_X_embedding_all` and stack all of those to create one $500 \\times 300$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feature_matrix_embedding(posts, vocab, embedding_table):\n",
    "    X_embedding_all = posts_to_embedding_matrix(posts, vocab, embedding_table)\n",
    "    return np.vstack([np.mean(X_post, axis=0) for X_post in X_embedding_all])\n",
    "\n",
    "train_X_embedding = construct_feature_matrix_embedding(train_unigrams, unigram_vocab, U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_embedding shape:                     (500, 300)\n",
      "# of non-zero entries in X_embedding:  144300\n",
      "% of non-zero entries in X_embedding:  96.20%\n"
     ]
    }
   ],
   "source": [
    "print(\"X_embedding shape:                     {0}\".format(train_X_embedding.shape))\n",
    "print(\"# of non-zero entries in X_embedding:  {0}\".format(np.sum(train_X_embedding != 0)))\n",
    "print(\"% of non-zero entries in X_embedding:  {0:.2f}%\".format(100 * np.mean(train_X_embedding != 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new representation is much smaller ($500 \\times 300$ vs. $500 \\times 40000$ish), but much denser. Overall, it has more non-zero entries than the bag-of-words matrix. The information is much less granular, since we took the mean of the word embeddings. If we're training a simple softmax regression model, that means we have to train one parameter per dimension per model:\n",
    "\n",
    "$\\theta_{1,1}, \\theta_{1,2}, ...\\theta_{4, 300}$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;or&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\theta_{\\text{Atheism},\\text{ Dim. 1}}, \\theta_{\\text{Atheism},\\text{ Dim. 2}}, ...\\theta_{\\text{Space}, \\text{ Dim. 300}}$\n",
    "\n",
    "That's way fewer than we had to train for the bag-of-words model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a simple model (or two)\n",
    "\n",
    "We now have two matrix representations of our posts. Let's try training a couple models and see which does better! We'll use sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_kwargs = {\n",
    "    'penalty': 'l2',\n",
    "    'C': 1.0,\n",
    "    'solver': 'lbfgs',\n",
    "    'multi_class': 'multinomial',\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_bow = LogisticRegression(**model_kwargs)\n",
    "F_bow.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_embedding = LogisticRegression(**model_kwargs)\n",
    "F_embedding.fit(train_X_embedding, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection\n",
    "\n",
    "So which of these models is better? They both trained without giving an error. We could compare how well they do on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words model training set accuracy: 97.60%\n",
      "Embedding model training set accuracy:    87.40%\n"
     ]
    }
   ],
   "source": [
    "bow_acc = F_bow.score(train_X, train_y)\n",
    "embedding_acc = F_embedding.score(train_X_embedding, train_y)\n",
    "\n",
    "print(\"Bag-of-words model training set accuracy: {0:.2f}%\".format(100 * bow_acc))\n",
    "print(\"Embedding model training set accuracy:    {0:.2f}%\".format(100 * embedding_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the bag-of-words model does much better on the training set. But maybe it's just overfit to that dataset and won't generalize well. The best measure of model performance is to score it on an unseen validation set. The 20 Newsgroups dataset has a test set, which we'll pretend is a validation set. In essence, they're the same. But we really shouldn't look at it until we want a final, final score. Side note: look how it easy it is after we wrote those functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "test_parsed, test_y = get_newsgroups_parsed('test')\n",
    "# Featurize\n",
    "test_unigrams, test_bigrams = zip(*[featurize_post(post) for post in test_parsed])\n",
    "# Build BoW matrix\n",
    "test_X = construct_feature_matrix(test_unigrams, test_bigrams, unigram_vocab, bigram_vocab)\n",
    "# Build embedding matrix\n",
    "test_X_embedding = construct_feature_matrix_embedding(test_unigrams, unigram_vocab, U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words model test set accuracy: 65.71%\n",
      "Embedding model test set accuracy:    68.00%\n"
     ]
    }
   ],
   "source": [
    "bow_test_acc = F_bow.score(test_X, test_y)\n",
    "embedding_test_acc = F_embedding.score(test_X_embedding, test_y)\n",
    "\n",
    "print(\"Bag-of-words model test set accuracy: {0:.2f}%\".format(100 * bow_test_acc))\n",
    "print(\"Embedding model test set accuracy:    {0:.2f}%\".format(100 * embedding_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that: the bag-of-words model is overfit. It does much better on the training set, but the embedding model does better on the test set. So how can we improve these models? The best way is certainly to use the full training set. But we could also tune hyperparameters. In `model_kwargs`, `C` describes the strength of the $\\ell_2$ regularizer. Increasing the strength (reducing `C`) could reduce overfitting for `F_bow`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
